{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wazoku data science backend developer test\n",
    "\n",
    "## Description\n",
    "This repo contains a method for measuring the similarity of description text associated with pairs of idea objects in a collection of idea objects. These similarities can be used to recommend related ideas to a focal idea. A command is included that can be used to populate a database with ideas having associated description text that consists of terms randomly sampled from a given vocabulary. Also included is a script named `save_tfidf_weights.py` which is run manually and saves a collection of term frequency-inverse document frequency (TF-IDF) weights for the descriptions associated with ideas. For each idea, a weight is stored for each term appearing in the vocabulary (see TF-IDF algorithm section below). TF-IDF weights are intended to reflect how important a term is to a document in a collection of documents. To simplfy this assessment, no pre-processing of text occurs prior to evaluating TF-IDF weights. Finally, a script called `save_recommendations.py` is included which is also run manually and saves a similarity for each pair of idea descriptions. These similarities are the cosine similarities of the TF-IDF vectors associated with the idea description (see Cosine similarity algorithm section below).\n",
    "\n",
    "## TF-IDF algorithm\n",
    "\n",
    "This section describes the particular TF-IDF algorithm used here (many variations are used in general). The vocabulary used for this exercise is a fixed set of terms. For each term, $t$, in the vocabulary, $V$, an inverse document frequency (IDF) is evaluated as follows:\n",
    "\n",
    "- Set $N$ to be the total number of ideas\n",
    "- Set $n_t$ to be the number of ideas that have a description containing term $t$ at least once\n",
    "- The IDF for term t is then given by \n",
    "\n",
    "$$\\mathrm{IDF}_t = 1 + \\log \\left(\\frac{N + 1}{n_t + 1}\\right)$$ where $\\log$ is the natural logarithm (base $e$). \n",
    "\n",
    "It is therefore the case that $\\mathrm{IDF}_t$ decreases as $n_t$ increases (so that \"rare\" terms have a relatively high IDF and \"common\" terms have a relatively low IDF).\n",
    "\n",
    "The normalised TF-IDF weight for idea $i$ and term $t$ is then evaluated as follows:\n",
    "\n",
    "- The term frequency (TF) for term $t$ in idea $i$ is the number of times $t$ appears in the description for $i$. Let us denote the TF for term t in idea i as $\\mathrm{TF}_{i, t}$.\n",
    "- The TF-IDF weight for idea $i$ and term $t$ is then given by \n",
    "\n",
    "$$\\mathrm{TFIDF}_{i, t} = \\mathrm{TF}_{i, t} \\cdot \\mathrm{IDF}_t$$\n",
    "\n",
    "- TF-IDF weights are then normalised so that the vector made from the TF-IDF weights for each idea has length $1$. The magnitude of the vector for idea $i$, denoted $M_i$, is given by \n",
    "\n",
    "$$M_i = \\sqrt{\\sum_{t \\in V} (\\mathrm{TFIDF}_{i, t})^2}$$\n",
    "\n",
    "- The normalised TF-IDF weight for idea $i$ and term $t$ is then given by \n",
    "\n",
    "$$\\mathrm{NTFIDF}_{i, t} = \\frac{\\mathrm{TFIDF}_{i, t}}{M_i}$$\n",
    "\n",
    "For each idea, we store the collection of normalised TF-IDF weights for all terms in the vocabulary.\n",
    "\n",
    "## Cosine similarity algorithm\n",
    "\n",
    "The cosine simlarity of two vectors is the cosine of the angle between the vectors. The TF-IDF weights for an idea can be represented as a vector with each element associated with a term in the vocabulary, so that the cosine similarity of the TF-IDF weights associated with a pair of ideas can be evaluated by finding the cosine of the angle between the two assocaited vectors. Since TF-IDF weights for an idea are already normalised, the cosine similarity of the TF-IDF weights associated with idea $i$ and those associated with idea $j$ is given by \n",
    "\n",
    "$$\\mathrm{similarity}(i, j) = \\sum_{t \\in V} \\mathrm{NTFIDF}_{i, t} \\cdot \\mathrm{NTFIDF}_{j, t}$$ \n",
    "\n",
    "## Getting started\n",
    "\n",
    "Copying the repository\n",
    "\n",
    "Due to the public nature of forks we suggest you duplicate the repo rather then forking it. \n",
    "You will need to create your own repo e.g. `[your_github_username]/wazoku_test` and then clone \n",
    "this repo `ben-may3031/wazoku_test` and push the code into your new one. You can follow the steps for doing this here: https://help.github.com/articles/duplicating-a-repository/\n",
    "\n",
    "Before proceeded be aware that this exercise assumes you are using a linux machine with [pip](https://pip.pypa.io/en/stable) and [virtualenv](https://virtualenv.pypa.io/en/stable/) installed. \n",
    "\n",
    "Create a new python 3.6 virtualenv in your checked out repo.\n",
    "\n",
    "    cd /[path_to]/wazoku_test\n",
    "    virtualenv -p python3.6 .\n",
    "\n",
    "\n",
    "Then install the dependencies:\n",
    "\n",
    "    bin/pip install -r requirements.txt\n",
    "\n",
    "\n",
    "Set the default django settings file used by all following commands:\n",
    "\n",
    "    export DJANGO_SETTINGS_MODULE=exercise.settings\n",
    "\n",
    "\n",
    "The code in this repo uses an sqlite database as the persistence layer. You can initialize an sqlite database (this db will be stored in the file `./db.sqlite3`)\n",
    "\n",
    "    bin/python manage.py migrate\n",
    "\n",
    "There is a simple django `populate_db` command which can be used to prime the database with some idea data (the descriptions for the ideas are 20 terms randomly sampled from the set vocabulary using an uneven probability distribution with duplicates allowed)\n",
    "\n",
    "    bin/python manage.py populate_db\n",
    "\n",
    "The script to save tfidf weights can be run with the following:\n",
    "\n",
    "    bin/python -m scripts.save_tfidf_weights\n",
    "    \n",
    "After the tfidf weights have been saved, the script to save recommendations can be run with the following:\n",
    "\n",
    "    bin/python -m scripts.save_recommendations\n",
    "\n",
    "There is also a unit test for each script and an integration test for both which can be used to validate the code:\n",
    "\n",
    "    bin/python manage.py test tests/*\n",
    "\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "Write a script to print the idea names for the top recommendations for a given idea. The script should take two arguments: (i) the pk of the idea to find recommendations for (ii) the number of recommendations to show. Recommendations should be printed in descending similarity order (i.e most similar printed first). The script should call two functions, one to evaluate the list to print, and one to print the list. Write a unit test to cover the functionality of the first of these functions.\n",
    "\n",
    "Please create a copy of the master branch and create a pull request into master for this work.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "The TF-IDF weights are stored in an inefficient form containing redundant information. Update the `save_tfidf_weights.py` script so that the `IdeaTfidfWeights` objects take up less disk space when stored, and (importantly) the `save_recommendations.py` script runs much faster. The recommendation similarities and hence results of the script you wrote for exercise 1 should be unaffected by these changes. You should not need to update tests for these changes.\n",
    "\n",
    "Please create a copy of the branch you used for exercise 1 and create a pull request into master for this work.\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "`Recommendation` objects are being saved that essentially contain the same information. Update the `save_recommendations.py` script and the script you wrote for exercise 1 so that half as many `Recommendation` objects are saved with the script you wrote for exercise 1 still printing the same results. You will need to update tests after these changes.\n",
    "\n",
    "Please create a copy of the branch you used for exercise 2 and create a pull request into master for this work.\n",
    "\n",
    "## Exercise 4\n",
    "\n",
    "If the number of ideas stored increased, then the amount of memory used by the `recommendation_objects` variable used for the `save_recommendations.py` script will increase too. Update the script so that the memory used for that variable is limited. You should not have to update tests for these changes.\n",
    "\n",
    "Please create a copy of the branch you used for exercise 3 and create a pull request into master for this work.\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
